{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환\n",
    "import os, re\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n"
     ]
    }
   ],
   "source": [
    "txt_dir = r\"C:\\Users\\Jennie\\Desktop\\aiffel\\Aiffel_MiniProject6\\Jennie_Aiffel\\lyrics\\*\"\n",
    "txt_list = glob.glob(txt_dir)\n",
    "\n",
    "raw_corpus = []\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read(). splitlines()\n",
    "        raw_corpus.extend(raw) #추가 내용 연장\n",
    "        # append, extend차이 https://m.blog.naver.com/wideeyed/221541104629\n",
    "print(\"데이터 크기: \", len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this isn't  sample sentence <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([.,¿])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    sentence = re.sub(r\"[;:@#$%&*.,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "  \n",
    "print(preprocess_sentence(\"This isn't ;;;sample    sentence.\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> looking for some education <end>', '<start> made my way into the night <end>', '<start> all that bullshit conversation <end>', \"<start> baby   can't you read the signs? i won't bore you with the details   baby <end>\", \"<start> i don't even wanna waste your time <end>\"]\n",
      "175749\n"
     ]
    }
   ],
   "source": [
    "# 정제 문장 모으기\n",
    "corpus = []\n",
    "# 정제안된 코퍼스 리스트에 저장된 문장들을 순서대로 반환하여 setence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence)==0: continue\n",
    "    if sentence[-1]==\":\": continue\n",
    "    # 앞서 구현한 정제 함수를 이용하여 문장 정제하고 담기\n",
    "    pre_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(pre_sentence)\n",
    "\n",
    "print(corpus[:5])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 개수 15개 이상 처리 전: 175749\n",
      "토큰 개수 15개 이상 처리 후: 160892\n",
      "[[  2 298  23 ...   0   0   0]\n",
      " [  2 217  11 ...   0   0   0]\n",
      " [  2  21  14 ...   0   0   0]\n",
      " ...\n",
      " [  2  24  67 ...   0   0   0]\n",
      " [  2  35  20 ...   0   0   0]\n",
      " [  2  24  67 ...   0   0   0]] <keras.preprocessing.text.Tokenizer object at 0x000002565A38C130>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화할때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=14000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    print(\"토큰 개수 15개 이상 처리 전:\", len(tensor))\n",
    "    for word in tensor:\n",
    "        if len(word)>=15:\n",
    "            tensor.remove(word)\n",
    "    print(\"토큰 개수 15개 이상 처리 후:\", len(tensor))\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  (160892, 14)\n",
      "[   2  298   23   88 5223    3    0    0    0    0    0    0    0    0]\n",
      "target sentence:  (160892, 14)\n",
      "[ 298   23   88 5223    3    0    0    0    0    0    0    0    0    0]\n",
      "(128713, 14)\n",
      "(32179, 14)\n",
      "(128713, 14)\n",
      "(32179, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 자연어처리 분야에서 모델의 입력이 되는 문장을 src ,\n",
    "# 정답 역할을 하게 될 모델의 출력 문장을 타겟 문장 tgt\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "print(\"source sentence: \", src_input.shape)\n",
    "print(src_input[0])\n",
    "tgt_input = tensor[:, 1:]\n",
    "print(\"target sentence: \", tgt_input.shape)\n",
    "print(tgt_input[0])\n",
    "\n",
    "# tokenize() 함수로 데이터를 Tensor로 변환한 후,\n",
    "# sklearn 모듈의 train_test_split() 함수를 사용해\n",
    "# 훈련 데이터와 평가 데이터를 분리\n",
    "# 단어장의 크기는 12,000 이상 으로 설정\n",
    "# 총 데이터의 20% 를 평가 데이터셋으로 사용하기\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, shuffle=True, random_state=42)\n",
    "print(enc_train.shape)\n",
    "print(enc_val.shape)\n",
    "print(dec_train.shape)\n",
    "print(dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        # Embedding layer, 2개 LSTM layer, 1개 Dense layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear1 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn1(out)\n",
    "        out = self.rnn2(out)\n",
    "        out = self.linear1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14001\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 1024\n",
    "hidden_size = 2048 \n",
    "vocab_size = tokenizer.num_words+1\n",
    "print(vocab_size)\n",
    "model = TextGenerator(vocab_size, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : the\n",
      "5 : i\n",
      "6 : you\n",
      "7 : and\n",
      "8 : to\n",
      "9 : a\n",
      "10 : me\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "  print(idx, \":\", tokenizer.index_word[idx])\n",
    "  if idx>=10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_status = model.load_weights(\"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본 문장생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해서 문장 만들기\n",
    "    while True:\n",
    "        # 1. 입력받은 문장의 텐서를 입력합니다\n",
    "        predict = model(test_tensor) \n",
    "        # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        # 3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "        # (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
    "        if predict_word.numpy()[0]==end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "    \n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "    return generated # 최종적으로 모델이 생성한 문장 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> all night long we're gonna rock this house until we knock it down <end> \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, tokenizer, init_sentence=\" <start>Who\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love you <end> \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, tokenizer, init_sentence=\" <start> I love\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 문장생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text2(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해서 문장 만들기\n",
    "    while True:\n",
    "        # 1. 입력받은 문장의 텐서를 입력합니다\n",
    "        predict = model(test_tensor) \n",
    "        # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "        predict = tf.nn.softmax(tf.squeeze(predict))\n",
    "        p_predict = np.array(predict[0])\n",
    "        p_predict /= p_predict.sum()\n",
    "        predict_word = np.random.choice(len(p_predict), size=1, p=p_predict)\n",
    "        predict_word = tf.convert_to_tensor(predict_word, dtype=tf.int64)\n",
    "        # 3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "        expand_word = tf.expand_dims(predict_word, axis=0)\n",
    "        test_tensor = tf.concat([test_tensor, expand_word], axis=-1)\n",
    "        # 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "        # (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
    "        if predict_word[0]==end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "    \n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "    return generated # 최종적으로 모델이 생성한 문장 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love ah if where every i (21st now but and <unk> please room \n"
     ]
    }
   ],
   "source": [
    "print(generate_text2(model, tokenizer, init_sentence=\" <start> I love \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love the someone upon i one i'm lift and memphis you feeling there's \n"
     ]
    }
   ],
   "source": [
    "print(generate_text2(model, tokenizer, init_sentence=\" <start> I love \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('python_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee5c845f7c411158c7d2a92f2c6170578906ecc5cb07bf65ac4ff23be2c9ca9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
